---
layout: archive
title: "3D reconstruction"
permalink: /news/
author_profile: true
redirect_from:
  - /resume
---
## [FADOR: Fast and Accurate Dynamic Object Removal for Indoor Scenes](https://zxczhai.github.io/FADOR/ "悬停显示")
[<img  align="left" src="https://github.com/nimtecv/nimtecv.github.io/raw/master//images/9.jpg"   width="1000px"  />](https://zxczhai.github.io/FADOR/ "悬停显示")
LiDAR point cloud is an important component of 3D reconstruction, and high-precision 3D point clouds provide accurate depth and scene structure. However, dynamic objects can form ghosting in the point cloud during data acquisition, causing data corruption. Most researchers have used the occlusion relationship between dynamic and static objects to achieve simple trajectory removal of dynamic objects in outdoor open scenes. However, in indoor scenes, due to the complex occlusion relationship, it is not possible to ensure the fine-grained, efficiency, and accuracy of the removal to achieve practical application levels. Here we proposed an offline point cloud dynamic object removal method based on time visibility. We believe that dynamic objects can be regarded as a more obvious noise point, and it is feasible to distinguish dynamic and static objects from the time dimension under the premise of their unstable observation. Therefore, we proposed a multi-stage method for dynamic object removal. Specifically, we use hash-encoded voxels to express local space, encode local space's time sequence occupancy count as a unified vector, and summarize comprehensive indicators of observation continuity and repeatability. Using this index to achieve dynamic object removal. In addition, we proposed a density-based binary classification method for more fine-grained removal tasks. Finally, we have validated the proposed algorithm's robustness, advancement, and efficiency in multiple challenging indoor scenes.


## [Dip-NeRF: Depth-Based Anti-Aliased Neural Radiance Fields](https://qinshihao12.github.io/Dip-NeRF/ "悬停显示")
[<img  align="left" src="https://github.com/nimtecv/nimtecv.github.io/raw/master//images/333.png"   width="1000px" />](https://qinshihao12.github.io/Dip-NeRF/ "悬停显示")
Neural radiation field (NeRF)-based novel view synthesis methods are gaining popularity for their ability to generate detailed and realistic images. However, most NeRF-based methods only use images to learn scene representations, ignoring the importance of depth information. The Zip-NeRF method has achieved impressive results in unbounded scenes by combining anti-aliasing techniques and mesh representations. However, the method requires a large number of input images and may perform poorly in complex scenes. Our method incorporates the advantages of Zip-NeRF and incorporates depth information to reduce the number of required images and solve the scale-free problem in borderless scenes. Experimental results show that our method effectively reduces the training time.And we can generate high-quality images and fine point cloud models using few images, even in complex scenes with numerous occlusions.


## [IPAL: Infinite Planes as Lines for Consistent Mapping in Indoor Multi-floor Environments](https://zxczhai.github.io/IPAL/) 
[<img  align="left" src="https://github.com/nimtecv/nimtecv.github.io/raw/master//images/jin.png"   width="1000px" />](https://zxczhai.github.io/IPAL/ "悬停显示") 
Indoor high-precision maps are harder to obtain. Numerous researchers considered the extendibility of indoor plane features to constrain the poses and improve mapping consistency. However, the usage of parametric planar or line features requires complex modeling, and threshold-based matching methods are eager to lead to degradation. Indeed, planes can be more simply represented as line projections, with the projection direction perpendicular to the plane’s normal vector. In this paper, we proposed a novel mapping optimization framework that incorporates building outline features specifically designed for multi-story buildings. The framework consists of three main components: LiDAR Bundle Adjustment(LBA), global constraints based on building outline features, and factor graph optimization. The method extracts the ground only for horizontal correction, while the core idea is based on the assumption of vertical structure, projecting the point cloud contours of multistory buildings to the top view perspective for independent spatial pose correlation, which is a strong constraint and very effective. Finally, constraints are constructed based on keyframes and intermediate frames are refined by a factor graph. Through extensive experiments conducted on multi-story buildings, we demonstrate that our algorithm significantly enhances mapping consistency and local accuracy compared to existing methods.
### Image Translation
## [DINO-ViT Enhanced Diffusion for Multi-exemplar-based Image Translation](https://littlefairy1.github.io/)
[<img  align="left" src="https://github.com/nimtecv/nimtecv.github.io/raw/master//images/07.jpg"   width="1000px" />](https://littlefairy1.github.io/ "悬停显示") 
We have developed a framework for multi-exemplar-based image translation. Most exemplar-based image translation methods allow only one target image for appearance transfer. In these existing methods, GANs are often used as generators, and features extracted from pre-trained CNNs are used as visual descriptors. By comparison, our framework allows users to provide one or more images as exemplars to realize the appearance transfer of different objects while preserving the structure of the source image. Methods based on GANs are typically limited to a specific domain. To overcome this, we choose a diffusion model as the generator in our framework, allowing image translation across arbitrary domains. DINO-ViT, as a visual transformer trained by self-supervision, its deep features have rich semantic properties and visual information compared to CNNs’. Therefore, we use the deep features extracted from pretrained DINO-ViT as visual descriptors to achieve more accurate image translation. The naive combination of translation results from different exemplars may lead to visual disharmony due to rough edges and differences in object visual characteristics. To address this, we introduce small amounts of noise on the combined image to reduce inconsistencies and use the reverse process of diffusion to denoise, then we can obtain an image that is almost identical to the combined image but without artifacts. Our framework offers higher-quality image translation and more flexible image editing, and we have demonstrated the effectiveness and superiority of our approach in several image translation tasks.
